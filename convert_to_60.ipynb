{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('env')",
   "metadata": {
    "interpreter": {
     "hash": "2b72526ed668422a4c42010915fcc77e6aba1ae6946ec448acf614b0bbb60243"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\nYour GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: GeForce RTX 2070 SUPER, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from flow_models import *\n",
    "from model_trainer import AnimeModel\n",
    "from tensorflow.keras import mixed_precision\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_size = (960,540)\n",
    "interp_ratio = [0.4,0.8]\n",
    "model_f = hr_3_2_16\n",
    "weight_dir = 'savedmodels/hr3216bilinear7/20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 540, 960, 6) 0                                            \n__________________________________________________________________________________________________\nHR_0 (HighResolutionModule)     [(None, 540, 960, 16 9712        input_1[0][0]                    \n__________________________________________________________________________________________________\nHR_1 (HighResolutionModule)     [(None, 540, 960, 16 52144       HR_0[0][0]                       \n__________________________________________________________________________________________________\nHR_2 (HighResolutionModule)     [(None, 540, 960, 16 268128      HR_1[0][0]                       \n                                                                 HR_1[0][1]                       \n__________________________________________________________________________________________________\nFusion_0 (HighResolutionFusion) [(None, 540, 960, 32 4064        HR_2[0][0]                       \n                                                                 HR_2[0][1]                       \n                                                                 HR_2[0][2]                       \n__________________________________________________________________________________________________\nactivation (Activation)         (None, 540, 960, 32) 0           Fusion_0[0][0]                   \n==================================================================================================\nTotal params: 334,048\nTrainable params: 331,744\nNon-trainable params: 2,304\n__________________________________________________________________________________________________\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x149ac1ea160>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "inputs = tf.keras.Input((frame_size[1],frame_size[0],6))\n",
    "anime_model = AnimeModel(inputs, model_f, interp_ratio)\n",
    "anime_model.load_weights(weight_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_dir = Path('data/cut')\n",
    "vid_paths = [str(vid_dir/vn) for vn in os.listdir(vid_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1600.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "695e0a08ca084730a76e2e47232813aa"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(vid_paths[0])\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "writer = cv2.VideoWriter(f'{vid_paths[0]}_interp.mp4',fourcc,60,frame_size)\n",
    "ret, frame = cap.read()\n",
    "for i in trange(1600):\n",
    "    if not cap.isOpened():\n",
    "        break\n",
    "    if ret:\n",
    "        frame0 = frame\n",
    "    else:\n",
    "        break\n",
    "    # ret, _ = cap.read()\n",
    "    # if not ret:\n",
    "    #     break\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frame1 = frame\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frame2 = frame\n",
    "    else:\n",
    "        break\n",
    "    frame0_resized = cv2.resize(frame0, dsize=frame_size)\n",
    "    frame1_resized = cv2.resize(frame1, dsize=frame_size)\n",
    "    frame2_resized = cv2.resize(frame2, dsize=frame_size)\n",
    "    concated1 = np.concatenate([frame0_resized,frame1_resized],axis=-1).astype(np.float32)/ 255.0\n",
    "    concated2 = np.concatenate([frame2_resized,frame1_resized],axis=-1).astype(np.float32)/ 255.0\n",
    "    outputs = anime_model(np.array([concated1,concated2]))\n",
    "    outputs = np.round(np.clip(outputs, 0, 1) * 255).astype(np.uint8)\n",
    "    interped1, interped2 = outputs[0][...,0:3], outputs[0][...,3:6]\n",
    "    interped3, interped4 = outputs[1][...,3:6], outputs[1][...,0:3]\n",
    "    writer.write(frame0_resized)\n",
    "    writer.write(interped1)\n",
    "    writer.write(interped2)\n",
    "    writer.write(interped3)\n",
    "    writer.write(interped4)\n",
    "\n",
    "\n",
    "cap.release()\n",
    "writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}